{"cells":[{"cell_type":"markdown","metadata":{"id":"kmK4UEj1-gHB"},"source":["# [![](https://img.shields.io/badge/Video-H∆∞·ªõng%20d·∫´n-ff0000)](https://youtu.be/798qchqWty4) [![](https://img.shields.io/badge/Design-stablediffusion.vn-0075ff)](https://stablediffusion.vn) [![](https://img.shields.io/badge/Ver-1.2-0075ff)](https://stablediffusion.vn/update/) [![](https://img.shields.io/badge/All%20Tools-sdvn.me-0075ff)](https://stablediffusion.vn/bo-cong-cu/) [![](https://img.shields.io/badge/SDVN-Library-green)](https://bit.ly/sdvn-lib) [![](https://img.shields.io/badge/Kho√°%20h·ªçc-SD%20Flux-red)](https://hungdiffusion.com/) [![](https://img.shields.io/badge/Group-Support-0075ff)](https://www.facebook.com/groups/stablediffusion.vn) [![](https://img.shields.io/discord/813085864355037235?color=blue&label=Discord&logo=Discord)](https://discord.gg/5SEtApPeyG)\n","---\n","üß® `Ch·ªâ c√≥ th·ªÉ train khi k·∫øt n·ªëi A100`"]},{"cell_type":"markdown","metadata":{"id":"2DRFiKdj5UWy"},"source":["#‚òïÔ∏è 1.C√†i ƒë·∫∑t"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"Gip8TXBMjVEP"},"outputs":[],"source":["#@title üíΩ Install\n","%cd /content\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","!pip install huggingface-sb3\n","from huggingface_hub import login\n","from huggingface_hub import HfApi\n","read_token = \"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\"\n","login(read_token, add_to_git_credential=True)\n","api = HfApi()\n","user = api.whoami(read_token)\n","\n","!git clone https://github.com/ostris/ai-toolkit.git\n","%cd ai-toolkit\n","!git submodule update --init --recursive\n","!pip install -q -r requirements.txt\n","\n","from transformers import AutoProcessor, AutoModelForCausalLM\n","from transformers import BlipProcessor, BlipForConditionalGeneration\n","from PIL import Image\n","import requests\n","import copy\n","import torch\n","%matplotlib inline\n","!pip install flash_attn timm"]},{"cell_type":"markdown","metadata":{"id":"4Xd6BfhD5YHU"},"source":["#‚è≥ 2. C·∫•u h√¨nh train"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"rWF9MkYhNR8b"},"outputs":[],"source":["#@title ‚ú® 2.1 X·ª≠ l√Ω d·ªØ li·ªáu\n","import os\n","\n","TrainFolder = \"/content/drive/MyDrive/SD-Data/TrainData\"  # @param {type:'string'}\n","DataClean = False #@param {type:\"boolean\"}\n","# @markdown üü° `ƒêi·ªÅu ch·ªânh m·ª©c ƒë·ªô d√†i caption t·ª± ƒë·ªông`\n","Caption = 'Florence' # @param ['None','Blip','Florence']\n","Caption_Length = \"Short\" # @param ['Short','Medium','Long']\n","# @markdown üü° `Th√™m caption tu·ª≥ ch·ªçn`\n","Custom_Caption = \"\" # @param {type:'string',placeholder:\"Nh·∫≠p t·ª´ kho√° tu·ª≥ ch·ªçn ho·∫∑c chu·ªói mu·ªën xo√° khi ch·ªçn ch·∫ø ƒë·ªô Remove\"}\n","Remove_Caption = False #@param {type:\"boolean\"}\n","Append = False #@param {type:\"boolean\"}\n","SubFolder = True\n","\n","extension =\".txt\"\n","\n","def clean_directory(directory):\n","  supported_types = [\".png\",\".jpg\",\".jpeg\",\".webp\",\".bmp\"]\n","  for item in os.listdir(directory):\n","      file_path = os.path.join(directory, item)\n","      if os.path.isfile(file_path):\n","          file_ext = os.path.splitext(item)[1]\n","          if file_ext not in supported_types:\n","              print(f\"Deleting file {item} from {directory}\")\n","              os.remove(file_path)\n","      elif os.path.isdir(file_path):\n","          clean_directory(file_path)\n","\n","\n","if DataClean == True :\n","    %cd /content\n","    clean_directory(TrainFolder)\n","\n","Cap_prompt = {\n","    'Short':['<CAPTION>',10,30],\n","    'Medium':['<DETAILED_CAPTION>',10,100],\n","    'Long':['<MORE_DETAILED_CAPTION>',10,150]\n","}\n","\n","if Caption == 'Blip':\n","  processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n","  model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n","if Caption == 'Florence':\n","  model_id = 'microsoft/Florence-2-large'\n","  model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True, torch_dtype='auto').eval().cuda()\n","  processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n","\n","def florence_caption(prompt,image):\n","    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to('cuda', torch.float16)\n","    generated_ids = model.generate(\n","      input_ids=inputs[\"input_ids\"].cuda(),\n","      pixel_values=inputs[\"pixel_values\"].cuda(),\n","      max_new_tokens=1024,\n","      early_stopping=False,\n","      do_sample=False,\n","      num_beams=3,\n","    )\n","    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n","    return generated_text\n","\n","def caption_dir(image_dir,prompt):\n","  cap = None\n","  for img_file in os.listdir(image_dir):\n","      file_path = os.path.join(image_dir, img_file)\n","      if os.path.isdir(file_path) :\n","          caption_dir(file_path,prompt)\n","      if img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n","          img_path = os.path.join(image_dir, img_file)\n","          image = Image.open(img_path)\n","          if Caption == 'Blip':\n","            inputs = processor(image, return_tensors=\"pt\")\n","            outputs = model.generate(**inputs, min_length=Cap_prompt[Caption_Length][1], max_length=Cap_prompt[Caption_Length][2])\n","            cap = processor.decode(outputs[0], skip_special_tokens=True)\n","          else:\n","            cap = florence_caption(prompt,image).replace('The image shows','')\n","          txt_path = os.path.join(image_dir, f\"{os.path.splitext(img_file)[0]}{extension}\")\n","          with open(txt_path, \"w\") as f:\n","              f.write(cap)\n","          print(f\"Mi√™u t·∫£ c·ªßa ·∫£nh {img_file}: {cap}\")\n","  return cap\n","if Caption != 'None':\n","  cap = caption_dir(TrainFolder,Cap_prompt[Caption_Length][0])\n","else:\n","  cap = ''\n","\n","def read_file(filename):\n","    with open(filename, \"r\") as f:\n","        contents = f.read()\n","    return contents\n","\n","def write_file(filename, contents):\n","    with open(filename, \"w\") as f:\n","        f.write(contents)\n","\n","def process_tags(filename, custom_tag, append, remove_tag):\n","    contents = read_file(filename)\n","    if remove_tag:\n","      contents = contents.replace(custom_tag, \"\")\n","    else:\n","      tags = [tag.strip() for tag in contents.split(',')]\n","      custom_tags = [tag.strip() for tag in custom_tag.split(',')]\n","      for custom_tag in custom_tags:\n","          custom_tag = custom_tag.replace(\"_\", \" \")\n","          if custom_tag not in tags:\n","              if append:\n","                  tags.append(custom_tag)\n","              else:\n","                  tags.insert(0, custom_tag)\n","      contents = ', '.join(tags)\n","    write_file(filename, contents)\n","\n","def check_dir(image_dir):\n","  if not any([filename.endswith(extension) for filename in os.listdir(image_dir)]):\n","      for filename in os.listdir(image_dir):\n","          if filename.endswith((\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\")):\n","              open(\n","                  os.path.join(image_dir, filename.split(\".\")[0] + extension),\n","                  \"w\",\n","              ).close()\n","\n","def process_dir(image_dir, tag, append, remove_tag):\n","  check_dir(image_dir)\n","  for filename in os.listdir(image_dir):\n","      file_path = os.path.join(image_dir, filename)\n","      if os.path.isdir(file_path) :\n","          print(filename)\n","          process_dir(file_path, tag, append, remove_tag)\n","      elif filename.endswith(extension):\n","          process_tags(file_path, tag, append, remove_tag)\n","\n","if Custom_Caption != \"\":\n","  process_dir(TrainFolder, Custom_Caption, Append, Remove_Caption)\n","\n","space = '' if cap =='' else ', '\n","Sample_Prompts = f'{Custom_Caption}{space}{cap}'\n","print()\n","print(f'Prompt Sample: \"{Sample_Prompts}\"')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"Z3vJmKYKNcVh"},"outputs":[],"source":["#@title ‚öôÔ∏è 2.2 C√†i ƒë·∫∑t train\n","Type = 'Dev' # @param ['Dev','Schnell']\n","Lora_name = \"flux_lora_name\" #@param {type:'string',placeholder:\"Nh·∫≠p t√™n lora\"}\n","OutputFolder = \"/content/drive/MyDrive/SD-Data/Lora\" #@param {type:'string',placeholder:\"/content/drive/MyDrive/SD-Data/Lora\"}\n","# @markdown üü° `ƒêi·ªÅu ch·ªânh steps theo m·ª©c ƒë·ªô train mong mu·ªën`\n","Steps = 1000 #@param {type:'number',placeholder:\"1000\"}\n","Save_steps = 250 #@param {type:'number',placeholder:\"250\"}\n","Batch_size = 4 #@param {type:'number',placeholder:\"4\"}\n","# @markdown üü° `C√°c th√¥ng s·ªë n√™n ƒë·ªÉ theo m·∫∑c ƒë·ªãnh`\n","Lr = 1e-4 #@param {type:'number',placeholder:\"1e-4\"}\n","Dim = 16 #@param {type:'number',placeholder:\"16\"}\n","Alpha = 16 #@param {type:'number',placeholder:\"16\"}\n","Resolution = \"[ 512, 768, 1024 ]\" #@param {type:'string', placeholder:\"[ 512, 768, 1024 ]\"}\n","\n","if Type == 'Dev':\n","  file_path = '/content/ai-toolkit/config/examples/train_lora_flux_24gb.yaml'\n","else:\n","  file_path = '/content/ai-toolkit/config/examples/train_lora_flux_schnell_24gb.yaml'\n","\n","import re\n","def replace(old_string, new_string):\n","    with open(file_path, 'r') as file:\n","        yaml_content = file.read()\n","    updated_content = re.sub(old_string, new_string, yaml_content)\n","    with open(file_path, 'w') as file:\n","        file.write(updated_content)\n","config = {\n","    'data': [r'folder_path:.*',f'folder_path: \"{TrainFolder}\"'],\n","    'name': [r'name:.*',f'name: \"{Lora_name}\"'],\n","    'output': [r'training_folder:.*',f'training_folder: \"{OutputFolder}\"'],\n","    'steps': [r' steps:.*',f' steps: {Steps}'],\n","    'save_steps': [r'save_every:.*',f'save_every: {Save_steps}'],\n","    'sample_every': [r'sample_every:.*',f'sample_every: {Save_steps}'],\n","    'resolution': [r'resolution:.*',f'resolution: {Resolution}'],\n","    'batch': [r'batch_size:.*',f'batch_size: {Batch_size}'],\n","    'dim': [r'linear:.*',f'linear: {Dim}'],\n","    'alpha': [r'linear_alpha:.*',f'linear_alpha: {Alpha}'],\n","    'lr': [r'lr:.*',f'lr: {Lr}'],\n","    'prompt': [r'prompts:[\\s\\S]*?neg:',f'prompts:\\n          - \"{Sample_Prompts}\"\\n        neg:']\n","}\n","for i in config:\n","  replace(config[i][0],config[i][1])\n","print('Done!')"]},{"cell_type":"markdown","metadata":{"id":"uDofc0SKJ5Fq"},"source":["#3. üß® Train"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"WUyXPXupBcO2"},"outputs":[],"source":["#@title ‚è≥ Run Lora Train\n","!python /content/ai-toolkit/run.py {file_path}"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["2DRFiKdj5UWy"],"gpuType":"A100","machine_shape":"hm","private_outputs":true,"provenance":[{"file_id":"1QNvU4CR_jwyyzgt-hmg2uPXS8MbrpBqI","timestamp":1718354635858}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
